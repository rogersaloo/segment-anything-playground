# 日本語のドキュメントを細かく調整する
## ファインチューニング SAM ドキュメント
このドキュメントには、コードのファインチューニングプロセスと本番用スクリプトの準備の説明が含まれています。rogers aloo [github](https://github.com/rogersaloo) が作成しました。SAMモデルのファインチューニングのフルレポは [こちら](https://github.com/rogersaloo/segment-anything-playground) を参照してください。

### 説明
この [リポジトリ](https://github.com/rogersaloo/segment-anything-playground) のルートにある ``official_sam_repo`` は、プロンプトによるセグメンテーションタスクのための基礎モデルを提供します。
このモデルのリリースは、明示的なファインチューニング機能を持たないまま行われました。このフォルダには、このモデルの微調整が含まれています。

このフォルダでは、マスクデコーダーに焦点を当て、微調整を行います。
微調整を行います。

segment_anything/predictor.pyの予測器SamPredctorは、基礎となるアーキテクチャの3つの部分（画像エンコード）をすべて含んでいるので、使用しないでください。
予測器には、基礎となるアーキテクチャの3つの部分（画像エンコーダ、プロンプトエンコーダ、マスクデコーダ）がすべて含まれているからです。さらに、SamPredictorクラスの "predict_torch" 
は勾配の再計算を防ぐデコレーター(@torch.no_grad)を持っています。

### ファインチューニング
通常のコンピュータビジョンの場合、学習データセットとは異なる新しい画像をモデルに見せようとすると、出力性能が低下します。
出力性能は低下します。ファインチューニングを行うことで、元のモデルに匹敵する性能を維持することができます。

ファインチューニングは、事前に訓練したモデルとその重みを取り込み、新しいデータセットや特定のタスクに適用することができます。
特定のユースケースのニーズに関連するタスクに適用することができます。

### 必要条件
モデルを微調整するためには、セグメンテーションする画像、画像に対する真実の根拠、画像に対するプロンプトが必要です。この例

## はじめに
### ヘルパー
#### 説明
[facebookresearch](https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb)が提供するヘルパー関数が含まれる。

#### マスクを表示する
```show_mask``` 画像中のマスクを表示します。

### ボックスの表示 
``show_box``` 画像中のオブジェクトに関連するバウンディングボックスを表示します。

## プロット
### 説明
このパートでは、グランドトゥルースとモデル後のマスク画像のプロットに焦点を当てます。

1. ``plot_ground_truth``： グランドトゥルースマスクをプロットする
2. ```plot_sam_vs_tuned```: サンプルのマスクとファインチューニングを比較した画像をプロットする(手と手を合わせて)
3. ``plot_train_mean``: トレーニングロスの平均グラフをプロットする。
4. ```compare_models_masks``` sam マスクとファインチューニングの画像マスクの比較プロットを比較する。

## プリプロセッシング_画像

SAMモデルにロードする前に、画像を前処理する。モジュールは、SAMモデルの入力に合うように画像サイズをリサイズする。

### モジュール
1. get_bounding_box_coordinates``：渡された画像のバウンディングボックスの座標。
2. 2. ``preprocess_image``： グランドトゥルースセグメンテーションマスクを抽出する
3. ``preprocess_image``: グラウンドトゥルースのセグメンテーションマスクを抽出します： 前処理済みの画像を返す 入力画像をフォーマット変換して画像サイズを変更する SAMの内部関数に期待すること


## トレイン
### 説明
SAMモデルの実行と微調整に使用されるメインスクリプトが含まれています。

### モジュール
1. ```compare_bbox_images_to_ground_truth_segmentation：``` バウンディングボックスを取得し、マスクのグランドトゥルースに対してプロットする。
2. ``get_ground_truth_masks``： グランドトゥルースボトルのマスクを取得します。
3. ``train_sam``： 微調整されたパラメータでsamモデルを訓練する
4. ``original_sam_model``： オリジナルのSAMモデルをダウンロードし、インスタンス化する
5. ```tuned_sam_model：``` チューニングされたSAMモデルをダウンロードし、インスタンス化する
6. ```load_image_to_predict```: 予測用の画像をロードする。
7. ```predict_on_tuned_sam```: チューニングされたSAMモデル。
8. ``predict_on_original_sam``: ロードする画像ID。
9. ``main()``： トレーニングモデルを実行する

## コマンド
ライブサーバを起動し、ロードするために実行します；
* `mkdocs build` - ドキュメントサイトを構築する。
* `mkdocs serve` - ライブでロードする docs サーバーを起動する。
* `mkdocs -h` - ヘルプメッセージを表示し、終了する。

## プロジェクトレイアウト

    mkdocs.yml # 設定ファイルです。
    docs/
        <!-- index.md # ドキュメントのトップページです。-->
        helpers.md # マスクをプロットするためのヘルパー関数
        plots.md # プロット 
        preprocess_image.md # samで使用するための画像の前処理
        main.md # 訓練とsamのメインモデル
        config.md # ハイパーパラメータや変数の細かい調整
        app.md # dockerizeしてfast api endpointを作る。
        japanese.md # ドキュメントの日本語訳